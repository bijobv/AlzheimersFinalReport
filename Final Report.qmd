---
title: "Final Report"
author: "Bijo Varghese, Hong Fu, Jessica Kentwell"
date: "`r format(Sys.Date(), '%B %d, %Y')`" # current date using r 
format:
    html:
        embed-resources: true
        page-layout: full
editor: visual
---

```{r echo=FALSE}
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse, caret, ranger, tree, glmnet, ISLR, ggplot2, Matrix, dplyr, knitr, kableExtra)

# read the cleaned Alzheimers dataset 
alzdata <- readRDS("knnalzdata.RDS")
```

```{r train}
set.seed(5003)
train_index <- createDataPartition(alzdata$Diagnosis, p = 0.7, list = FALSE)
train_data <- alzdata[train_index, ]
test_data <- alzdata[-train_index, ]

modelcv <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  search = "grid", 
  classProbs = TRUE,
  summaryFunction = prSummary, 
  savePredictions = "all",
)
#model example for knn
###set.seed(5003)
###knn_model <- train(
 ### Diagnosis ~ .,
 ###data = train_data,
 ### method = "knn",
  #tuneLength = 30,
 ###trControl = modelcv,
 ### preProcess = c("center", "scale"),
 #metric = "AUC"

```

## Lasso Regression Model

```{r, lasso}
# lasso regression model requires caret and glmnet packages
library(caret)
library(glmnet)
library(MLmetrics)
library(dplyr)

# scaling train_data and test_data for lasso as lasso is sensitive to unscaled data
target <- train_data$Diagnosis
predictors <- train_data %>% select(-Diagnosis)
scaled_predictors <- as.data.frame(scale(predictors))
train_data <- cbind(scaled_predictors, Diagnosis=target)
# scaling test data 
target <- test_data$Diagnosis
predictors <- test_data %>% select(-Diagnosis)
scaled_predictors <- as.data.frame(scale(predictors))
test_data <- cbind(scaled_predictors, Diagnosis=target)


# setup a range of lambda values 
tune_grid <- expand.grid(alpha = 1, # alpha 1 denotes lasso regression model 
                         lambda = seq(0.001, 0.1, length = 10))

# Train the model
lasso_model <- train(
  Diagnosis ~ ., 
  data = train_data,
  method = "glmnet",
  tuneGrid = tune_grid,
  trControl = modelcv,
  )

# print the best lambda value
print(lasso_model$bestTune)

```

```{r, lasso_performance}
## lasso confusion matrix 
lasso_predictions <- predict(lasso_model, newdata = test_data)
lasso_CM <- confusionMatrix(lasso_predictions, test_data$Diagnosis, positive = "Alzheimers")
print(lasso_CM)

#with adjusted probabilities
lasso_probs <- predict(lasso_model, newdata = test_data, type = "prob")
lasso_predictions_2 <- ifelse(lasso_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
lasso_predictions_2 <- factor(lasso_predictions_2, levels = c("No_Alzheimers", "Alzheimers"))
lasso_CM_2 <- confusionMatrix(lasso_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lasso_CM_2)


```

```{r}
#regular
#knn_predictions <- predict(knn_model, newdata = test_data)
#knn_CM <- confusionMatrix(knn_predictions, test_data$Diagnosis, positive = "Alzheimers")
#print(knn_CM)

#with adjusted probabilities
#knn_probs <- predict(knn_model, newdata = test_data, type = "prob")
#knn_predictions_2 <- ifelse(knn_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
#knn_predictions_2 <- factor(knn_predictions_2, levels = c("No_Alzheimers", "Alzheimers"))
#knn_CM_2 <- confusionMatrix(knn_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
#print(knn_CM_2)
```

## Random Forest

```{r rfB, message=FALSE, echo=FALSE, warning=FALSE}
ntree_seq <- c(1, 50, 100, 250, 500)
max.ntree_seq <- max(ntree_seq)
test_diagnosis <- test_data[["Diagnosis"]]



rand.forest.function <- function(x) { 
  
  rf_model <- train(
  Diagnosis ~ .,              
  data = train_data,
  method = "rf",
  trControl = modelcv,
  tuneGrid = expand.grid(.mtry = sqrt(ncol(train_data) - 1)), 
  ntree = x
  )
  
  predict_rf <- predict(rf_model, newdata = test_data, type = "prob")
  predicted_labels <- ifelse(predict_rf[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers") |> as.factor()
  CM <- confusionMatrix(test_diagnosis, predicted_labels, positive = "Alzheimers")
  Sensit <- CM$byClass[["Sensitivity"]]
  Specif <- CM$byClass[["Specificity"]]
  Accura <- CM$overall[["Accuracy"]]
  TP <- CM$table["Alzheimers", "Alzheimers"]
  FP <- CM$table["Alzheimers", "No_Alzheimers"]
  FN <- CM$table["No_Alzheimers", "Alzheimers"]
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2 * (precision * recall) / (precision + recall)
  Performance_measures <- list(ntrees = x, Sensitivity = Sensit, Specificity = Specif, Accuracy = Accura, F1 = F1)
  }
perf_measures <- map(ntree_seq, rand.forest.function)
sensitivities <- map_dbl(perf_measures, "Sensitivity")
plot_data <- data.frame(ntree_seq, sensitivities)
ggplot(plot_data, aes(x = ntree_seq, y = sensitivities)) +
  geom_line() +
  geom_point() + 
  geom_text(aes(label = round(sensitivities, 4)), 
            vjust = -0.5, 
            color = "black", 
            size = 3.5) +
  labs(x = "Number of Trees", y = "Sensitivity") +
  theme_minimal()


perf_measures_df <- bind_rows(perf_measures)

perf_measures_long <- perf_measures_df %>%
  pivot_longer(cols = c(Sensitivity, Specificity, Accuracy, F1),
               names_to = "Measure",
               values_to = "Value")

ggplot(perf_measures_long, aes(x = factor(ntrees), y = Value, fill = Measure)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.2f", Value)), 
            position = position_dodge(width = 0.8), 
            vjust = -0.5, 
            size = 2.5) +
  labs(x = "Number of Trees", y = "Value") +
  scale_fill_manual(values = c("#99A88C", "#335C67", "#E09F3E", "#E3DED1")) + 
  theme_minimal()

```

**Performance by number of trees**
``` {r foresttable, echo=FALSE, message=FALSE}
kable(perf_measures_df, format = "html", digits = 2) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

### Overview of the problem

### Dataset Description

### Initial Data Analysis / Visualisation of the data

### Feature Engineering

### Classification Algorithms used

### Classification Performance Evaluation

### Conclusion
