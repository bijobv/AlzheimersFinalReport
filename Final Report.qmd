---
title: "Final Report"
author: "Bijo Varghese, Hong Fu, Jessica Kentwell"
date: "`r format(Sys.Date(), '%B %d, %Y')`" # current date using r 
format:
    html:
        embed-resources: true
        page-layout: full
editor: visual
css: styles.css
---

```{r setup, echo=FALSE}
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse, caret, ranger, tree, glmnet, ISLR, ggplot2, Matrix, dplyr, knitr, kableExtra, MASS, RColorBrewer, caTools, MLmetrics, lattice, psych, ggcorrplot, readr, tidyr, ROCR, ROCit)


# read the cleaned Alzheimers dataset 
alzdata <- readRDS("knnalzdata.RDS")
ogalzdata <- readRDS("ogalzdata.RDS")
```

```{r train, echo=FALSE, message=FALSE}
set.seed(5003)
train_index <- createDataPartition(alzdata$Diagnosis, p = 0.7, list = FALSE)
train_data <- alzdata[train_index, ]
test_data <- alzdata[-train_index, ]

modelcv <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  search = "random", #or grid
  classProbs = TRUE,
  summaryFunction = prSummary, 
  savePredictions = "final",
)


```

## Lasso Regression Model

```{r lasso, echo=FALSE, message=FALSE}
# setup a range of lambda values 
tune_grid <- expand.grid(alpha = 1, # alpha 1 denotes lasso regression model 
                         lambda = seq(0.001, 0.1, length = 10))

# Train the model
lasso_model <- train(
  Diagnosis ~ ., 
  data = train_data,
  method = "glmnet",
  tuneGrid = tune_grid,
  trControl = modelcv,
  preProcess = c("center", "scale")
  )

# print the best lambda value
print(lasso_model$bestTune)

```

```{r lasso_performance, echo=FALSE, message=FALSE}

#with adjusted probabilities
lasso_probs <- predict(lasso_model, newdata = test_data, type = "prob")
lasso_predictions_2 <- ifelse(lasso_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
lasso_predictions_2 <- factor(lasso_predictions_2, levels = c("No_Alzheimers", "Alzheimers"))
lasso_CM_2 <- confusionMatrix(lasso_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lasso_CM_2)


```

## kNN Model

```{r knn, warning=TRUE, echo=FALSE, message=FALSE}
#KNN model train
set.seed(5003)
knn_model <- train(
Diagnosis ~ .,
data = train_data,
method = "knn",
tuneLength = 10,
trControl = modelcv,
preProcess = c("center", "scale"),
metric = "F"
)
```

```{r knn_predict_2, echo=FALSE, message=FALSE}
knn_probs <- predict(knn_model, newdata = test_data, type = "prob")
knn_preds_2 <- ifelse(knn_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
knn_preds_2 <- factor(knn_preds_2, levels = c("No_Alzheimers", "Alzheimers"))
knn_CM_2 <- confusionMatrix(knn_preds_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(knn_CM_2)
```

## LDA Model

```{r lda, warning=FALSE}
#LDA model train
set.seed(5003)
lda_model <- train(
  Diagnosis ~ .,
  data = train_data,
  method = "lda",
  trControl = modelcv,
  preProcess = c("center", "scale"),
  metric = "F"
)
```

```{r lda_predict_2, echo=FALSE, message=FALSE}
lda_prob_predictions <- predict(lda_model, newdata = test_data, type = "prob")
#new threshold
new_prob <- 0.35
lda_predictions_2 <- ifelse(lda_prob_predictions$Alzheimers >= new_prob, "Alzheimers", "No_Alzheimers")
lda_predictions_2 <- factor(lda_predictions_2, levels = levels(test_data$Diagnosis))
lda_CM_2 <- confusionMatrix(lda_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lda_CM_2)
```

## Random Forest

```{r rfB, message=FALSE, echo=FALSE, warning=FALSE, eval=FALSE}
ntree_seq <- c(1, 50, 100, 250, 500)
max.ntree_seq <- max(ntree_seq)
mtry_values <- seq(1, ncol(train_data) - 1, by = 5)
test_diagnosis <- test_data[["Diagnosis"]]

#rand.forest.function <- function(x) { 
  
  rf_model <- train(
  Diagnosis ~ .,              
  data = train_data,
  method = "rf",
  trControl = modelcv,
  tuneLength = 10,
  tuneGrid = expand.grid(.mtry = mtry_values), 
 # ntree = x
  )
  
  rf_probs <- predict(rf_model, newdata = test_data, type = "prob")[, "Alzheimers"]
  
  predict_rf <- predict(rf_model, newdata = test_data, type = "prob")
  predicted_labels <- ifelse(predict_rf[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers") |> as.factor()
  CM <- confusionMatrix(test_diagnosis, predicted_labels, positive = "Alzheimers")
  Sensit <- CM$byClass[["Sensitivity"]]
  Specif <- CM$byClass[["Specificity"]]
  Accura <- CM$overall[["Accuracy"]]
  TP <- CM$table["Alzheimers", "Alzheimers"]
  FP <- CM$table["Alzheimers", "No_Alzheimers"]
  FN <- CM$table["No_Alzheimers", "Alzheimers"]
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2 * (precision * recall) / (precision + recall)
  Performance_measures <- list(ntrees = x, Sensitivity = Sensit, Specificity = Specif, Accuracy = Accura, F1 = F1)
  }
perf_measures <- map(ntree_seq, rand.forest.function)
sensitivities <- map_dbl(perf_measures, "Sensitivity")
plot_data <- data.frame(ntree_seq, sensitivities)
ggplot(plot_data, aes(x = ntree_seq, y = sensitivities)) +
  geom_line() +
  geom_point() + 
  geom_text(aes(label = round(sensitivities, 4)), 
            vjust = -0.5, 
            color = "black", 
            size = 3.5) +
  labs(x = "Number of Trees", y = "Sensitivity") +
  theme_minimal()


#perf_measures_df <- bind_rows(perf_measures)

#perf_measures_long <- perf_measures_df %>%
 # pivot_longer(cols = c(Sensitivity, Specificity, Accuracy, F1),
#               names_to = "Measure",
 #              values_to = "Value")

#ggplot(perf_measures_long, aes(x = factor(ntrees), y = Value, fill = Measure)) +
  #geom_bar(stat = "identity", position = "dodge") +
 # geom_text(aes(label = sprintf("%.2f", Value)), 
  #          position = position_dodge(width = 0.8), 
 #           vjust = -0.5, 
 #           size = 2.5) +
#  labs(x = "Number of Trees", y = "Value") +
#  scale_fill_manual(values = c("#E09F3E", "#335C67", "#99A88C", "#E3DED1")) + 
#  theme_minimal()

```

**Performance by number of trees**

```{r foresttable, echo=FALSE, message=FALSE}
#kable(perf_measures_df, format = "html", digits = 2) %>%
#  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

### 

## Overview of the Problem

It is estimated that 30% of older adults aged 60 and above die from Alzheimer's disease, a common and progressive neurodegenerative disorder that primarily affects memory, thinking, and behavior. Alzheimer's is the leading cause of dementia, a term used to describe severe cognitive decline that interferes with daily life. The exact cause of Alzheimer’s is not fully understood, as it involves a complex interaction of genetic, environmental, and lifestyle factors. Typically, Alzheimer's begins with mild memory loss, especially affecting recent memories, but it progressively worsens, leading to more severe symptoms such as confusion, language difficulties, impaired problem-solving, mood swings, personality changes, and challenges with daily tasks.

One of the critical issues with Alzheimer's is the lack of a definitive diagnostic test; diagnosis is currently based on clinical history and observed symptoms. Early detection is essential, yet without a conclusive diagnostic method, it remains challenging. To address this, it is vital to develop an effective diagnostic approach that can identify early symptoms using readily accessible patient data and medical histories.

Our primary goal in this business case is to develop an accurate predictive model capable of classifying individuals as having or not having Alzheimer’s based on clinical patient data. Additionally, we aim to identify the specific factors or combinations of factors that can reliably aid in predicting Alzheimer’s disease. Lastly, we will address challenges related to the available dataset to improve the data-capturing process, enhancing the quality and robustness of future datasets.

The ability to identify, detect and prevent, is crucial to sustain growing aging population in our societies. We believe our model can be the foundation to help people with early detection and intervention, and for governments to reduce the cost burden on Medicare.

## Alzheimer's Disease Dataset

We selected a dataset from Kaggle.com to develop our predictive model. This dataset is highly comprehensive and contains synthetic data, offering extensive health information for **`r nrow(alzdata)`** patients, each uniquely identified with IDs ranging from 4751 to 6900. The dataset includes demographic details, lifestyle factors, medical history, clinical measurements, cognitive and functional assessments, symptoms, and a diagnosis of Alzheimer's Disease.

With this richness in features, we have many variables to consider for our model. Notably, 65% of the patients are diagnosed as not having Alzheimer’s disease, while 35% are classified as having the disease. This dataset is ideal for researchers and data scientists aiming to explore factors associated with Alzheimer's, develop predictive models, and perform in-depth statistical analyses.

The table below provides a breakdown of these variables in its raw form.

```{r, data_description, fold: true, echo=FALSE, message=FALSE}

# Read the CSV file into a dataframe
#data <- read_csv("dataset_description.csv", show_col_types = FALSE)

#kable(data, format = "html") %>%
#  kable_styling() %>%
#  column_spec(1:ncol(data), extra_css = "font-size: 11px;") %>%  # Font size for table body
 # row_spec(0, extra_css = "font-size: 11px;")  # Font size for headers (row 0)
```

### Initial Data Analysis / Visualisation of the data - Bijo

Initial data analysis suggested that we had a full dataset with no missing values. As *Diagnosis* is our classification variable, we conducted our initial analysis around it. In figure 1, we can see how the dataset is imbalanced towards patients diagnosed with **No Alzheimers**. We will share later how the imbalance in the data will be addressed in our classification performance evaluation. Through a Pearson correlation matrix, in Figure 2, we also observed that the features have negligible colinearility between them. Finally, to observe the distribution of the diagnosis for each feature, we explored a violin plot in Figure 3, that suggested a normal distribution for most of features - except for MMSE, Functional Assessment and ADL. Figure 3 also illustrates that there are no obvious outliers or anomalous values due to measurement errors, data entry errors, or rare events were detected. Identifying and managing such outliers is crucial, as they could skew model performance if the model learns from extreme values, potentially hindering its ability to generalize effectively.

##### Figure 1. Percentage of frequencies in each class

```{r echo = FALSE}
diagnosis_counts <- table(alzdata$Diagnosis)
diagnosis_proportions <- prop.table(diagnosis_counts)

par(mai = c(1, 1, 0.5, 0.2))
imbalance_barplot <- barplot(diagnosis_proportions * 100,
  col = c("#E09F3E", "#335C67"),
  ylab = "Percentage",
  ylim = c(0, 100),
  border = NA,
  cex.names = 1,
  cex.axis = 1,
  cex.lab = 1,
  width = 0.5,
  space = 0.1,
  names.arg = c("No Alzheimers", "Alzheimer's")
)

text(imbalance_barplot, diagnosis_proportions * 100 + 4, 
     labels = paste0(round(diagnosis_proportions * 100, 1), "%"),
     cex = 0.8, col = "black")

abline(h = 0, col = "black", lwd = 1)
```

##### Figure 2. Correlation heatmap of numeric predictor variables

```{r echo=FALSE}

# Calculate correlation matrix and p-value matrix
numeric_vars <- names(ogalzdata)[sapply(ogalzdata, is.numeric)]
cor_results <- corr.test(ogalzdata[, numeric_vars], method = "pearson")

numeric_cor_matrix <- cor_results$r
numeric_p_matrix <- cor_results$p

# Plot with ggcorrplot
ggcorrplot(numeric_cor_matrix, 
           method = "square", 
           type = "lower", 
           lab = TRUE,  # Show correlation values
           lab_size = 0, 
           tl.cex = 8,  # Text label size
           tl.col = "black",
           p.mat = numeric_p_matrix, 
           sig.level = 0.05,  # Significance level for p-values
           insig = "blank",  # Mark non-significant correlations
           pch = 1,  # Symbol for non-significant
           colors = c("#E09F3E", "#E3DED1", "#335C67"))


```

##### Figure 3. Numeric predictor variables by diagnosis

```{r violinplot, echo=FALSE, error=FALSE, warning=FALSE}
# long format
numeric_vars_long <- reshape2::melt(ogalzdata, id.vars = "Diagnosis", measure.vars = numeric_vars)

# violin plots for numeric variables by target_var
ggplot(numeric_vars_long, aes(x = Diagnosis, y = value, fill = Diagnosis)) +
  geom_violin(trim = FALSE) +
  geom_boxplot(width = 0.1, fill = "white", outlier.shape = NA) +
  facet_wrap(~ variable, scales = "free_y") +  # Creates a facet for each variable
  labs(title = NULL, x = NULL, y = NULL) +
  theme_minimal() +
  scale_fill_manual(values = c("#E09F3E", "#335C67")) +
  theme(
    plot.title = element_text(size = 8),            
    strip.text = element_text(size = 8),            
    axis.text = element_text(size = 8),
    legend.position = "right")
```

### Feature Engineering - Jess

To prepare the dataset for training and testing, the raw data was transformed into a consistent format and saved as an RDS file. All features were converted to numeric values and standardized to have a mean of 0 and a standard deviation of 1. The target variable was converted into a factor with two levels: 'Alzheimer' and 'No Alzheimer'. This standardization ensures that all four classifiers use the same dataset for training, testing, and performance evaluation. Feature engineering details are discussed further in this report. The predictor variables "EducationLevel" and "Ethnicity" were dummy encoded to maintain the conversion to a numeric format. This is especially important for models like kNN, LDA and Lasso that require a numeric format.

Our dataset had some class imbalance, with 65% of patients not having Alzheimer's and 35% with Alzheimer's disease. Class imbalance can adversely impact model performance when it comes to predicting the minority class (ref). To lessen this impact, we adjusted the default probability threshold from 0.5 to 0.35 for classifying a patient as having Alzheimer's disease.

### Classification Algorithms used - All

We decided to utilise 4 models - random forest (RF), logistic regression with Lasso regularisation (Lasso), linear discriminant analysis (LDA) and k-nearest neighbours (kNN) to help us solve our classification problem, each with it's own unique strengths. *brief description about each one here*.

To ensure generalisation of our chosen models to the test data, we used 10-fold cross-validation with 10 repeats. We implemented this by using a common trainControl object (from caret) that used a grid search for the hyperparameter tuning grid. This allows us to compare fairly our models performance as the cross-validation folds have remained consistent. We initially set our tuneLength = 10 to let caret generate the hyperparameter settings. To determine the variability anad consistency of our models on the cross validated folds, we used the resamples function to visualise and compare their performance.

Each model had it's own hyperparameters to tune. **Lasso Model** For the Lasso model, we optimize the lambda parameter to control the regularization strength. High lambda values lead to simpler models by shrinking coefficients to zero, while low values retain more predictors. We use cross-validation with different lambda values to find the best balance between bias and variance. To automate this process, we perform a grid search across a range of 10 lambda values (0.001 to 0.1) to identify the optimal choice. Since Lasso is sensitive to feature scales, standardizing the data is essential. Finally, we refit the model on the full training set using the chosen lambda for final predictions. This approach ensures a well-regularized, generalized model suited to the task.

For the Lasso model, we need to optimise the lambda parameter. High lambda values lead to simpler models by shrinking coefficients to zero, while low values retain more predictors. We use cross-validation with different lambda values to find the best balance between bias and variance. The alpha hyperparameter was set to 1 which means a total Lasso. - **Random Forest**: We also trained and tested a decision tree model to inspect the region where a patient belongs and predict the most commonly occurring class in that region. To improve these predictions, we extended the tree to a Random Forest by training our model on an ensemble of trees - 1, 50, 100, 250 and 500. To further tune the model, we also trained the model on various mtry parameters - 1, 6, 11, 16, 21, 26, 31 and 36, to find the best m predictors at each split. - **kNN**: k. **LDA**: No hyperparameter tuning required.

### Classification Performance Evaluation - Jess

In order to evaluate the performance of our models, we utilised a variety of metrics to help us choose the best model. **AUC** (Area Under the ROC Curve) It measures a model’s ability to differentiate between classes, if the value if high, it means it has high classification power. **Recall/Sensitivity** (True Positive Rate): It indicates what percentage of actual positive cases had been correctly identified by the model. For our problem, it is the measure of how well the model is able to classify patients with Alzheimer's disease. A Recall score of 1.0 represents a perfect classifier; lower values represent more false negatives e.g., misclassifying an Alzheimer's patient as being other than one (non-Alzheimer's) **Precision** (Positive Predictive Value): The proportion of positive predictions that are correctly classified with denote. Lower Precision raises the false positives rate — incorrectly identifying a person without Alzheimer’s as having Alzheimer’s. **F1 score**: Mean of precision and recall.

Our cross-validation results across 100 samples demonstrate that RF is the top-performing model, with the highest AUC (0.94), Precision, Recall and F1 scores. LDA and Lasso models showed similar performances (mean AUC 0.92 for both), and kNN had the lowest mean AUC (0.85) as well as all other metrics. These results can be summarised in Figure 4, which also illustrates that the rf model has the lowest score variability out of all models, which indicates the model is not overfitting on the training data (ref).

```{r}
resamples_results <- resamples(list(KNN = knn_model, LDA = lda_model, RF = rf_model, Lasso = lasso_model))
#summary(resamples_results)

bwplot(resamples_results, metric = c("Recall", "F1", "AUC", "Precision"))
#dotplot(resamples_results, metric = c("Recall", "F1", "AUC", "Precision"))

```

To evaluate the models performance on the test data, we examined the confusion matrices and the ROC and Precision-Recall curves for each of the models. The results can be summarised in Table 3 and Figure 5. Similarly to the results on the cross validation folds, the RF model outperforms the 3 other models across all metrics. The RF model correctly identified 212 patients with Alzheimer's disease, and 396 without the disease. 16 patients without Alzheimer's were incorrectly classified as having it, and 20 patients with Alzheimer's disease were missed by the model. The high Recall/Sensitivity () indicates the RF model is excellent at identifying and detecting Alzheimer's disease patients. High Precision also means that those who were identified by our model as having the disease really do have it, minimising false positives. The F1 score is also high, showing a good balance between Precision and Recall. Furthermore, the model also shows high Specificity, which means it can also correctly identify patients with no Alzheimer's disease.

The LDA model correctly identified 191 patients with Alzheimer's but incorrectly classified 77 patients who didn't have Alzheimer's as having it. The Precision (71.27%) was particuarly low, indicating a higher rate of false positives compared to the RF model. The Recall (83.77%) and Specificty (81.49%) results were acceptable, with the lower F1 score (77.02%) taking into account the lower Precision. This model is therefore less reliable in classification than the RF model.

The Lasso model performs similalry to the LDA model with average performance, but higher Recall/Sensitivity (85.53%), meaning it can detect Alzheimer's patients better compared to the LDA model (it identified 195 patients with Alzheimer's correctly compared to the 191 correctly identified from the LDA model). However, similarly to the LDA model, it has low Precision (70.65%) leading to more false positives. The Lasso model incorrectly identified 81 patients who don't have Alzheimer's as having Alzheimer's.

The KNN model had the lowest performance among all models. The low Precision (60.47%) and F1 score (68.32%) indicates it had a high number of false positives (classifyiing 117 patients with no Alzheimer's as having Alzheimer's).

Comparing the performance of the models can also be confirmed statistically through McNemar's Test, which assesses if there is a significant difference in error rates between models. Low p-values suggest significant difference in performance.

```{r}
CMb4TA <- data.frame(
  Model = c("Lasso", "KNN", "LDA", "RF"),
  Accuracy = c(0.70, 0.76, 0.83, 0.95),
  Recall = c(0.70, 0.44, 0.74, 0.91),
  Precision = c(0.82, 0.79, 0.77, 0.95),
  F1_Score = c(0.76, 0.57, 0.76, 0.93),
  stringsAsFactors = FALSE
)

performance_df <- data.frame(
  Model = rep(c("Lasso", "kNN", "LDA", "Random Forest"), each = 2),
  Threshold = rep(c("Before Adjustment", "After Adjustment"), times = 4),
  Accuracy = c(0.70, 0.76, 0.83, 0.95, 0.82, 0.83, 0.94, 0.95),
  Recall = c(0.70, 0.44, 0.74, 0.91, 0.84, 0.74, 0.91, 0.91),
  Precision = c(0.82, 0.79, 0.77, 0.95, 0.71, 0.77, 0.93, 0.95),
  F1_Score = c(0.76, 0.57, 0.76, 0.93, 0.77, 0.76, 0.92, 0.93),
  stringsAsFactors = FALSE
)

library(tidyr)

performance_wide <- pivot_wider(
  performance_df,
  names_from = Threshold,
  values_from = c(Accuracy, Recall, Precision, F1_Score)
)
library(reactable)

# Create the reactable table
reactable(
  performance_wide,
  columns = list(
    Model = colDef(name = "Model", align = "left", minWidth = 150),
    Accuracy_Before_Adjustment = colDef(
      name = "Accuracy (Before)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#a6cee3")
    ),
    Recall_Before_Adjustment = colDef(
      name = "Recall (Before)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#b2df8a")
    ),
    Precision_Before_Adjustment = colDef(
      name = "Precision (Before)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#fb9a99")
    ),
    F1_Score_Before_Adjustment = colDef(
      name = "F1 Score (Before)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#fdbf6f")
    ),
    # Metrics after adjustment
    Accuracy_After_Adjustment = colDef(
      name = "Accuracy (After)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#1f78b4")
    ),
    Recall_After_Adjustment = colDef(
      name = "Recall (After)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#33a02c")
    ),
    Precision_After_Adjustment = colDef(
      name = "Precision (After)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#e31a1c")
    ),
    F1_Score_After_Adjustment = colDef(
      name = "F1 Score (After)",
      align = "center",
      minWidth = 130,
      cell = data_bars(performance_wide, fill_color = "#ff7f00")
    )
  ),
  bordered = TRUE,
  highlight = TRUE,
  striped = TRUE,
  defaultColDef = colDef(
    headerStyle = list(fontWeight = "bold")
  ),
  theme = reactableTheme(
    )
)



#CM_long <- CM_allmodels %>%
#  pivot_longer(
#    cols = c("Accuracy", "Recall", "Precision", "F1_Score"),
 #   names_to = "Metric",
 #   values_to = "Value"
 # )

#ggplot(CM_long, aes(x = Model, y = Value, fill = Model)) +
 # geom_bar(stat = "identity") +
  #facet_wrap(~ Metric, ncol = 2)  +
 # labs(title = "Model Performance Metrics", y = "Value") +
 # theme_minimal() +
 # theme(legend.position = "none",
   #     plot.title = element_text(hjust = 0.5, face = "bold"),
    #    strip.text = element_text(face = "bold"))


```

```{r}
library(precrec)
rf_probs <- predict_rf
lda_probs <- lda_prob_predictions

rf_probs <- predict(rf_model, newdata = test_data, type = "prob")[, "Alzheimers"]
lda_probs <- predict(lda_model, newdata = test_data, type = "prob")[, "Alzheimers"]
knn_probs <- predict(knn_model, newdata = test_data, type = "prob")[, "Alzheimers"]
lasso_probs <- predict(lasso_model, newdata = test_data, type = "prob")[, "Alzheimers"]

true_labels <- ifelse(test_data$Diagnosis == "Alzheimers", 1, 0)

models_probs_list <- list(
  "Random Forest" = rf_probs,
  "LDA" = lda_probs,
  "Lasso" = lasso_probs,
  "KNN" = knn_probs
)


mmdata_obj <- mmdata(scores = models_probs_list, labels = true_labels, modnames = names(models_probs_list))
eval_results <- evalmod(mmdata_obj)
plot(eval_results, curvetype = "ROC", main = "ROC Curves for KNN, LDA, Lasso and RF models")
plot(eval_results, curvetype = "PRC", main = "Precision-Recall Curves for All Models")

auc_values <- auc(eval_results)
print(auc_values)
```

The RF model also shows superior performance when analysing the ROC curves for the models. This is demonstrated by the curve being closest to the top left corner, which represents perfect classiifcation (Sensitivity = 1).

#### Feature Importance

```{r}
rf_var_imp <- varImp(rf_model)

plot(rf_var_imp, top = 10, main = "Top 10 Important Features from RF Model")

```

The top features extracted from the RF model include Functional Assessment and ADL scores. These features are neurocognitive assessments performed by a clinician and help measure cognitive function and daily living abilities. These are directly related to Alzheimer's disease. The RF model suggesting these features are important in our model is consistent with medical understanding about Alzheimer's.

## Discussion

###### Threshold adjustment

Adjusting our probability threshold to 0.35 (compared to 0.5) was due to our class imbalance, with only 35% of our positive class (Alzheimer's disease) being represented in the data. Analysing the impact on the results compared to when we didn't adjust the threshold (Table 3), we can see that the adjustment improved the Sensitivity/Recall across all of our models. However, it also appears to have increased the false positive rate (lower Precision) at the same time. In the context of our classification problem, we believe that identifying more actual Alzheimer's patients correctly outweighs the cost of a false positive. Early detection is vital in treating and managing Alzheimer's disease.

### Conclusion - Bijo Varghese

The best model

Why RF performed better.

Limitations: synthetic data, etc

Future research Data - ensemble methods

Medical - test on real patient data, consult with clinical experts.

Appendix
