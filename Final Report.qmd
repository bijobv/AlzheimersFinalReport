---
title: "Final Report"
author: "Bijo Varghese, Hong Fu, Jessica Kentwell"
date: "`r format(Sys.Date(), '%B %d, %Y')`" # current date using r 
format:
    html:
        embed-resources: true
        page-layout: full
editor: visual
---

```{r setup, echo=FALSE}
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse, caret, ranger, tree, glmnet, ISLR, ggplot2, Matrix, dplyr, knitr, kableExtra, MASS, RColorBrewer, caTools, MLmetrics)


# read the cleaned Alzheimers dataset 
alzdata <- readRDS("knnalzdata.RDS")
```

```{r train, echo=FALSE, message=FALSE}
set.seed(5003)
train_index <- createDataPartition(alzdata$Diagnosis, p = 0.7, list = FALSE)
train_data <- alzdata[train_index, ]
test_data <- alzdata[-train_index, ]

modelcv <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  search = "random", #or grid
  classProbs = TRUE,
  summaryFunction = prSummary, 
  savePredictions = "final",
)


```

## Lasso Regression Model

```{r lasso, echo=FALSE, message=FALSE}
# setup a range of lambda values 
tune_grid <- expand.grid(alpha = 1, # alpha 1 denotes lasso regression model 
                         lambda = seq(0.001, 0.1, length = 10))

# Train the model
lasso_model <- train(
  Diagnosis ~ ., 
  data = train_data,
  method = "glmnet",
  tuneGrid = tune_grid,
  trControl = modelcv,
  preProcess = c("center", "scale")
  )

# print the best lambda value
print(lasso_model$bestTune)

```

```{r lasso_performance, echo=FALSE, message=FALSE}

#with adjusted probabilities
lasso_probs <- predict(lasso_model, newdata = test_data, type = "prob")
lasso_predictions_2 <- ifelse(lasso_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
lasso_predictions_2 <- factor(lasso_predictions_2, levels = c("No_Alzheimers", "Alzheimers"))
lasso_CM_2 <- confusionMatrix(lasso_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lasso_CM_2)


```

## kNN Model

```{r knn, warning=TRUE, echo=FALSE, message=FALSE}
#KNN model train
set.seed(5003)
knn_model <- train(
Diagnosis ~ .,
data = train_data,
method = "knn",
tuneLength = 10,
trControl = modelcv,
preProcess = c("center", "scale"),
metric = "F"
)
```

```{r knn_predict_2, echo=FALSE, message=FALSE}
knn_probs <- predict(knn_model, newdata = test_data, type = "prob")
knn_preds_2 <- ifelse(knn_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
knn_preds_2 <- factor(knn_preds_2, levels = c("No_Alzheimers", "Alzheimers"))
knn_CM_2 <- confusionMatrix(knn_preds_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(knn_CM_2)
```

## LDA Model

```{r lda, warning=FALSE}
#LDA model train
set.seed(5003)
lda_model <- train(
  Diagnosis ~ .,
  data = train_data,
  method = "lda",
  trControl = modelcv,
  preProcess = c("center", "scale"),
  metric = "F"
)
```

```{r resamples, eval=FALSE}
#this is just for cross validated data - now we can go back and choose the metric we want to use and keep it consistent. F1 i think is the best. 
resamples_results <- resamples(list(KNN = knn_model, LDA = lda_model, RF = rf_model, Lasso = lasso_model))
summary(resamples_results)

bwplot(resamples_results, metric = c("Recall", "F1", "AUC"))
dotplot(resamples_results, metric = c("Recall", "F1", "AUC"))
```

```{r lda_predict_2, echo=FALSE, message=FALSE}
lda_prob_predictions <- predict(lda_model, newdata = test_data, type = "prob")
#new threshold
new_prob <- 0.35
lda_predictions_2 <- ifelse(lda_prob_predictions$Alzheimers >= new_prob, "Alzheimers", "No_Alzheimers")
lda_predictions_2 <- factor(lda_predictions_2, levels = levels(test_data$Diagnosis))
lda_CM_2 <- confusionMatrix(lda_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lda_CM_2)
```

## Random Forest

```{r rfB, message=FALSE, echo=FALSE, warning=FALSE}
ntree_seq <- c(1, 50, 100, 250, 500)
max.ntree_seq <- max(ntree_seq)
test_diagnosis <- test_data[["Diagnosis"]]



rand.forest.function <- function(x) { 
  
  rf_model <- train(
  Diagnosis ~ .,              
  data = train_data,
  method = "rf",
  trControl = modelcv,
  tuneLength = 10,
  tuneGrid = expand.grid(.mtry = sqrt(ncol(train_data) - 1)), 
  ntree = x
  )
  
  predict_rf <- predict(rf_model, newdata = test_data, type = "prob")
  predicted_labels <- ifelse(predict_rf[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers") |> as.factor()
  CM <- confusionMatrix(test_diagnosis, predicted_labels, positive = "Alzheimers")
  Sensit <- CM$byClass[["Sensitivity"]]
  Specif <- CM$byClass[["Specificity"]]
  Accura <- CM$overall[["Accuracy"]]
  TP <- CM$table["Alzheimers", "Alzheimers"]
  FP <- CM$table["Alzheimers", "No_Alzheimers"]
  FN <- CM$table["No_Alzheimers", "Alzheimers"]
  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2 * (precision * recall) / (precision + recall)
  Performance_measures <- list(ntrees = x, Sensitivity = Sensit, Specificity = Specif, Accuracy = Accura, F1 = F1)
  }
perf_measures <- map(ntree_seq, rand.forest.function)
sensitivities <- map_dbl(perf_measures, "Sensitivity")
plot_data <- data.frame(ntree_seq, sensitivities)
ggplot(plot_data, aes(x = ntree_seq, y = sensitivities)) +
  geom_line() +
  geom_point() + 
  geom_text(aes(label = round(sensitivities, 4)), 
            vjust = -0.5, 
            color = "black", 
            size = 3.5) +
  labs(x = "Number of Trees", y = "Sensitivity") +
  theme_minimal()


perf_measures_df <- bind_rows(perf_measures)

perf_measures_long <- perf_measures_df %>%
  pivot_longer(cols = c(Sensitivity, Specificity, Accuracy, F1),
               names_to = "Measure",
               values_to = "Value")

ggplot(perf_measures_long, aes(x = factor(ntrees), y = Value, fill = Measure)) +
  geom_bar(stat = "identity", position = "dodge") +
  geom_text(aes(label = sprintf("%.2f", Value)), 
            position = position_dodge(width = 0.8), 
            vjust = -0.5, 
            size = 2.5) +
  labs(x = "Number of Trees", y = "Value") +
  scale_fill_manual(values = c("#E09F3E", "#335C67", "#99A88C", "#E3DED1")) + 
  theme_minimal()

```

**Performance by number of trees**

```{r foresttable, echo=FALSE, message=FALSE}
kable(perf_measures_df, format = "html", digits = 2) %>%
  kable_styling(full_width = FALSE, bootstrap_options = c("striped", "hover", "condensed"))
```

### 

## Overview of the Problem

In our project, using anonymised patient history and assessments, we intend to create a right sized model with adequate performance to diagnose if the patient has Alzheimer's or not - binary classification. According to Alzheimer's association, 1 in 3 older adult dies with Alzheimer's or another form of dementia (Alzheimer's association, 2024). These statistics suggest that we have or will encounter people in our lives who suffer from some form of dementia. To date, the exact cause of Alzheimer's is not fully understood, but researchers have identified that vascular, genetics, and lifestyle & environmental factors contribute to its development. Therefore, early detection is critical to treat or prevent Alzheimer's. The ability to identify, detect and prevent, is crucial to sustain growing aging population in our societies. We believe our model can be the foundation to help people with early detection and intervention, and for governments to reduce the cost burden on Medicare.

## Alzheimer's Disease Dataset

There are **`r nrow(alzdata)`** observations and **`r ncol(alzdata)`** variables in this dataset, of which **`r (ncol(alzdata)-1)`** are independent variables and **'1'** is the target variable. The table below provides a breakdown of sample of these variables.

```{r, data_description, fold: true, echo=FALSE, message=FALSE}

# Load required libraries
library(readr)   # To read CSV files
library(knitr)   # To format tables in a nice layout
library(kableExtra)

# Read the CSV file into a dataframe
data <- read_csv("dataset_description.csv", show_col_types = FALSE)

kable(data, format = "html") %>%
  kable_styling() %>%
  column_spec(1:ncol(data), extra_css = "font-size: 11px;") %>%  # Font size for table body
  row_spec(0, extra_css = "font-size: 11px;")  # Font size for headers (row 0)
``` 


### Initial Data Analysis / Visualisation of the data - Bijo

### Feature Engineering - Jess

-   Dummy encoded multi-level categorical variables
-   No missing data
-   Kept all the key features given the medical nature of the problem
-   Scaled and centered the data, important especially for Lasso and kNN
-   Converted binary factors to numeric for kNN and Lasso

### Classification Algorithms used - All

-   Repeat cross-validation
    -   Resamples - chose F1
-   Hyperparameter tuning per model (everyone to contribute)
    -   Final model specs:
        -   KNN best tuned k =
        -   RF best tuned mtry = and ntree =
        -   Lasso best tuned lambda = and alpha =
        -   LDA (no hyperparameter tuning).

### Classification Performance Evaluation - Jess

-   Sensitivity
-   Threshold adjustment
-   Confusion matrix heatmap for each model as well as metrics: Sensitivity and F1.
-   ROC curves for each model.

### Conclusion - Bijo
