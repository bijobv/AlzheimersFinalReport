---
title: "Final Report"
author: "Bijo Varghese, Hong Fu, Jessica Kentwell"
date: "`r format(Sys.Date(), '%B %d, %Y')`" # current date using r 
format:
    html:
        embed-resources: true
        page-layout: full
editor: visual
---

```{r setup, echo=FALSE}
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman"); library(pacman)
pacman::p_load(tidyverse, caret, ranger, tree, glmnet, ISLR, ggplot2, Matrix, dplyr, knitr, kableExtra, MASS, RColorBrewer, caTools, MLmetrics, ROCit, ROCR, MLeval, plotscaper, precrec, parameters, see, performance, modelbased, cutpointr, performanceEstimation, reactablefmtr)


# read the cleaned Alzheimers dataset 
alzdata <- readRDS("knnalzdata.RDS")
```

```{r train, echo=FALSE, message=FALSE}
set.seed(5003)
train_index <- createDataPartition(alzdata$Diagnosis, p = 0.7, list = FALSE)
train_data <- alzdata[train_index, ]
test_data <- alzdata[-train_index, ]

modelcv <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10,
  search = "grid", #or random
  classProbs = TRUE,
  summaryFunction = prSummary, 
  savePredictions = "all",
)


```

## Lasso Regression Model

```{r lasso, echo=FALSE, message=FALSE}
# setup a range of lambda values 
#tune_grid <- expand.grid(alpha = 1, lambda = seq(0.001, 0.1, length = 10))

# Train the model
set.seed(5003)
lasso_model <- train(
  Diagnosis ~ ., 
  data = train_data,
  method = "glmnet",
  #tuneGrid = tune_grid,
  trControl = modelcv,
  tuneLength = 10,
  preProcess = c("center", "scale"),
  metric = "F"
  )

# print the best lambda value
print(lasso_model$bestTune)

```

```{r}
#no adjusted threshold
#lasso_preds <- predict(lasso_model, newdata = test_data)
lasso_probs <- predict(lasso_model, newdata = test_data, type = "prob")[, "Alzheimers"]
lasso_CM <-  confusionMatrix(lasso_probs, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lasso_CM)

```

```{r lasso_performance, echo=FALSE, message=FALSE}

#with adjusted probabilities

lasso_probs <- predict(lasso_model, newdata = test_data, type = "prob")[, "Alzheimers"]
lasso_predictions_2 <- ifelse(lasso_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
lasso_predictions_2 <- factor(lasso_predictions_2, levels = c("No_Alzheimers", "Alzheimers"))
lasso_CM_2 <- confusionMatrix(lasso_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lasso_CM_2)


```

## kNN Model

```{r knn, warning=TRUE, echo=FALSE, message=FALSE}
#KNN model train
set.seed(5003)
knn_model <- train(
Diagnosis ~ .,
data = train_data,
method = "knn",
tuneLength = 10,
trControl = modelcv,
preProcess = c("center", "scale"),
metric = "F"
)
```

```{r knn_predict_2, echo=FALSE, message=FALSE}
#knn_preds <- predict(knn_model, newdata = test_data)
knn_probs <- predict(knn_model, newdata = test_data, type = "prob")[, "Alzheimers"]
#knn_CM <- confusionMatrix(knn_preds, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
#knn_CM2 <- confusionMatrix(knn_preds_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
#knn_preds_2 <- ifelse(knn_probs[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers")
#knn_preds_2 <- factor(knn_preds_2, levels = c("No_Alzheimers", "Alzheimers"))
#knn_CM_2 <- confusionMatrix(knn_preds_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(knn_CM)
```

## LDA Model

```{r lda, warning=FALSE}
#LDA model train
set.seed(5003)
lda_model <- train(
  Diagnosis ~ .,
  data = train_data,
  method = "lda",
  trControl = modelcv,
  preProcess = c("center", "scale"),
  metric = "F"
)
```

```{r}
#lda_preds <- predict(lda_model, newdata = test_data)
lda_probs <- predict(lda_model, newdata = test_data, type = "prob")[, "Alzheimers"]
#lda_CM <- confusionMatrix(lda_preds, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
#print(lda_CM)
```

```{r lda_predict_2, echo=FALSE, message=FALSE}
lda_prob_predictions <- predict(lda_model, newdata = test_data, type = "prob")
#new threshold
new_prob <- 0.35
lda_predictions_2 <- ifelse(lda_prob_predictions$Alzheimers >= new_prob, "Alzheimers", "No_Alzheimers")
lda_predictions_2 <- factor(lda_predictions_2, levels = levels(test_data$Diagnosis))
lda_CM_2 <- confusionMatrix(lda_predictions_2, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
print(lda_CM_2)
```

## Random Forest

```{r rfB, message=FALSE, echo=FALSE, warning=FALSE}
ntree_seq <- c(1, 50, 100, 250, 500)
max.ntree_seq <- max(ntree_seq)
test_diagnosis <- test_data[["Diagnosis"]]

rf_model <- train(
  Diagnosis ~ .,              
  data = train_data,
  method = "ranger",
  trControl = modelcv,
  tuneLength = 10,
  preProcess = c("center", "scale"),
  #tuneGrid = expand.grid(.mtry = sqrt(ncol(train_data) - 1)), 
  #ntree = x
  metric = "F"
)

rf_probs <- predict(rf_model, newdata = test_data, type = "prob")[, "Alzheimers"]
#predict_rf <- predict(rf_model, newdata = test_data)
#rf_CM_noadjustment <- confusionMatrix(predict_rf, test_data$Diagnosis, positive = "Alzheimers", mode = "everything")
 # predicted_labels <- ifelse(predict_rf[, "Alzheimers"] > 0.35, "Alzheimers", "No_Alzheimers") |> as.factor()
 # rf_CM <- confusionMatrix(test_diagnosis, predicted_labels, positive = "Alzheimers", mode = "everything")


```

```{r resamples, eval=FALSE}
#this is just for cross validated data - now we can go back and choose the metric we want to use and keep it consistent. F1 i think is the best. 
resamples_results <- resamples(list(KNN = knn_model, LDA = lda_model, RF = rf_model, Lasso = lasso_model))
summary(resamples_results)

bwplot(resamples_results, metric = c("Recall", "F1", "AUC", "Precision"))
dotplot(resamples_results, metric = c("Recall", "F1", "AUC", "Precision"))
```

```{r}
library(precrec)

true_labels <- ifelse(test_data$Diagnosis == "Alzheimers", 1, 0)

models_probs_list <- list(
  "KNN" = knn_probs,
  "LDA" = lda_probs,
  "Lasso" = lasso_probs,
  "Random Forest" = rf_probs
)

name_list <- c("KNN", "LDA", "Lasso", "RF")
#models_labels_list <- list(knn = knn_labels, lda = lda_labels, lasso = lasso_labels, rf = rf_labels)

mmdata <- mmdata(scores = models_probs_list, labels = true_labels, modnames = name_list)
mmprecrec <- evalmod(mmdata)
plot(mmprecrec, curvetype = "ROC", main = "ROC Curves for KNN, LDA, Lasso and RF models")

roc_df <- as.data.frame(mmprecrec, curvetype = "ROC")


#

#autoplot(mmprecrec, "ROC")
#autoplot(mmprecrec, "PR")


#mmdata_pr <- mmdata(scores = models_probs_list, labels = true_labels)


#roc <- roc(true_labels, lda_probs)

```

```{r}
CM_allmodels <- data.frame(
  Model = c("Lasso", "KNN", "LDA", "RF"),
  Accuracy = c(0.70, 0.76, 0.83, 0.95),
  Recall = c(0.70, 0.44, 0.74, 0.91),
  Precision = c(0.82, 0.79, 0.77, 0.95),
  F1_Score = c(0.76, 0.57, 0.76, 0.93),
  stringsAsFactors = FALSE
)

CM_long <- CM_allmodels %>%
  pivot_longer(
    cols = c("Accuracy", "Recall", "Precision", "F1_Score"),
    names_to = "Metric",
    values_to = "Value"
  )

ggplot(CM_long, aes(x = Model, y = Value, fill = Model)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ Metric, ncol = 2)  +
  labs(title = "Model Performance Metrics", y = "Value") +
  theme_minimal() +
  theme(legend.position = "none",
        plot.title = element_text(hjust = 0.5, face = "bold"),
        strip.text = element_text(face = "bold"))


```

**Performance by number of trees** \#`{r foresttable, echo=FALSE, message=FALSE} #kable(perf_measures_df, format = "html", digits = 2)  #kable_styling(full_width = FALSE, bootstrap_options = c("striped", #"hover", "condensed")) #`

### Overview of the problem - Hong

### Dataset Description - Hong

### Initial Data Analysis / Visualisation of the data - Bijo

### Feature Engineering - Jess

-   Dummy encoded multi-level categorical variables
-   No missing data
-   Kept all the key features given the medical nature of the problem
-   Scaled and centered the data, important especially for Lasso and kNN
-   Converted binary factors to numeric for kNN and Lasso

### Classification Algorithms used - All

-   Repeat cross-validation
    -   Resamples - chose F1
-   Hyperparameter tuning per model (everyone to contribute)
    -   Final model specs:*t*
        -   KNN best tuned k =
        -   RF best tuned mtry = and ntree =
        -   Lasso best tuned lambda = and alpha =
        -   LDA (no hyperparameter tuning).

### Classification Performance Evaluation - Jess

-   Sensitivity
-   Threshold adjustment
    -   0.35
-   Confusion matrix heatmap for each model as well as metrics: Sensitivity and F1.
-   ROC curves for each model.

### Conclusion - Bijo

::: callout-note
We utilised k-fold cross-validation to ensure our chosen models performance will generalise to the test data. We used 10-fold cross-validation with 10 repeats as a common *TrainControl* object in `caret`{=texinfo} where we utilised a 'grid' search. When training our models individually, we began with utilising a common tuneLength = 10 and not specifying a performance metric. We utilised the caret::resamples function to assess the variability of our models across the cross-validated folds. These folds can be plotted as curves Appendix 1 shows the resampling results across tuning parameters.

###### Performance metrics for hyperparameter tuning

4 metrics are provided from resamples to tune our parameter grid. AUC (Area Under the Curve) measures a model's ability to differentiate between classes, with a higher score indicating greater classification ability. *Recall* (referred interchangeably, *Sensitivity or True Positive Rate*) is the probability of correctly identifying a positive case. In the context of our problem, it is the model's ability to classify an Alzheimer's disease patient with Alzheimer's disease. A recall score of 1.0 is a perfect classifier, with lower values leading to more false negatives (identifying an Alzheimer's patient as no Alzheimer's). Precision (Positive Predictive Value) is the number of correct diagnoses out of all the patients our model classified as having Alzheimer's disease. Lower precision increases the rate of false positives.
:::

::: callout-note
We initially only focused on Recall as the metric, however, the resamples data suggested that by using Recall as a metric for hyperparameter tuning, some of our models may have been overfitting (scores obtained were 1.0) (see Appendix 1). When interpreting the resamples output, any high standard deviation or variability between metric scores can indicate overfitting. We therefore decided to refine the tunelength to tuneGrid and tune for the F1 metric across all models, offering a balanced approach and a focus on consistency rather than the maximum value.

Figure 4 demonstrates that our random forest model outperformed the others on all metrics on the cross-validation folds training.

The final hyperparameters for each model is shown in Table 2.

Resampling results across tuning parameter *k* showed that as k increased, AUC increased, Precision decreased, Recall increased and F1 decreased.

Our tuning strategy
:::
